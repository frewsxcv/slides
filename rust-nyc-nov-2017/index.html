<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url('https://fonts.googleapis.com/css?family=Lato|Inconsolata');

      body {
        font-family: 'Lato';
      }

      .remark-slide-content h1, h2, h3, h4, h5, h6 {
        font-family: 'Lato';
        font-weight: normal;
      }

      .remark-slide-content h1 {
        font-size: 3rem;
      }

      .remark-slide-content h2 {
        font-size: 2.4rem;
      }

      .remark-code, .remark-inline-code {
        font-family: 'Inconsolata';
        font-size: 1.6rem;
        white-space: pre-wrap;
      }

      .footnote {
        font-size: 0.8em;
      }

      .small .remark-code {
        font-size: 110%;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

what the government doesn't want you to know about threadpools

10 ways threadpools can suck the life out of you

the rise of threadpools and how to make it stop

threadpools die every minute you don't read this article

11 ways investing in threadpools can make you a millionaire

7 things lady gaga has in common with threadpools

8 things the media hasn't told you about threadpools

???

hello everyone, thanks all for coming.

the main focus of my talk to night is on threadpools in rust. i wasn't clever enough to come up with a witty talk title so here's a bunch of clickbait-generated headlines for threadpools instead.

---

class: center, middle

# threads

# threadpools

# implementation

# usage

???

# threads

* how to use them in rust

# threadpools

* why we need threadpools

# implementation

* basic implementation

# usage

* brief overview of existing threadpool implementations

---

class: center, middle

# who

## corey farwell

## rwell.org

## @frewsxcv

???

who am i?

**corey farwell**

backend software engineer at kickstarter

ruby / rails

started using rust late 2014

contributed to various projects over the years

rust documentation team (w/ steve)

**rwell.org**

there's not much on it so don't bother going to it

**frewsxcv**

unpronouncible online handle

twitter github

---

class: center, middle

# https://slides.rwell.org

???

**read**: in case you want to follow along, here's a url to the slides

---

???

so why are we all here today? we're here to talk about rust. and what is rust?

---

class: center, middle

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Rust_programming_language_black_logo.svg/1024px-Rust_programming_language_black_logo.svg.png" width="60%">

???

rust is a programming language!

and what is the goal of this programming language?

---

class: middle

<em>

**What is this project's goal?**

To design and implement a safe, concurrent, practical systems language.

Rust exists because other languages at this level of abstraction and efficiency are unsatisfactory. In particular:

1. There is too little attention paid to safety.
1. They have poor concurrency support.
1. There is a lack of practical affordances.
1. They offer limited control over resources.

Rust exists as an alternative that provides both efficient code and a comfortable level of abstraction, while improving on all four of these points.

</em>

.footnote[â€” https://www.rust-lang.org/en-US/faq.html]

???

according to the FAQs on the rust website, the goal of rust is...

*To design and implement a safe, concurrent, practical systems language.*

in particular, for this talk, we're going to focus on the 'concurrent' aspect

---

class: center, middle

# â€œconcurrentâ€

???

wikipedia defines concurrent computing to be:

*a form of computing in which several computations are executed during overlapping time periodsâ€”concurrentlyâ€”instead of sequentially (one completing before the next starts).*

what is the basic building block of concurrency in rust?

---

class: center, middle

# threads!

---

# threads

```rust
use std::thread;
use std::time::Duration;

fn main() {
    thread::spawn(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from a thread! ðŸ‘‹")
    });
}
```

???

**explain**: go through every bit of the code

lets see what happens when we compile and run this...

---

# threads

```rust
use std::thread;
use std::time::Duration;

fn main() {
    thread::spawn(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from a thread! ðŸ‘‹")
    });
}
```

```sh
corey@mac /p/tmp> rustc code.rs
corey@mac /p/tmp> ./code
corey@mac /p/tmp>
```

???

what happened? where's our output?

some of you in the audience might have caught it earlier.

in the main thread, we called `thread::spawn` and a child thread spawned. but then the main thread reached the end of the function. when that happens, the main thread kills all child threads and the child thread never printed the output.

so how do we get the main thread to wait for the child thread to finish?

---

# threads

```rust
use std::thread;
use std::time::Duration;

fn main() {
    let handle = thread::spawn(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from a thread! ðŸ‘‹")
    });

    handle.join().unwrap();
}
```

???

`thread::spawn` returns a `JoinHandle` struct. the `JoinHandle` struct allows us to block the main thread and wait for the child thread to complete via the `join` method. now when we run it...

---

# threads

```rust
use std::thread;
use std::time::Duration;

fn main() {
    let handle = thread::spawn(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from a thread! ðŸ‘‹")
    });

    handle.join().unwrap();
}
```

```sh
corey@mac /p/tmp> rustc code.rs
corey@mac /p/tmp> ./code
hello from a thread! ðŸ‘‹
```

???

cool, looks like it worked

so now that we've mastered spawing a single thread, let's try spawning multiple

---

# multiple threads

```rust
let mut handles = vec![];

for i in 0..4 {
    handles.push(
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(1));
            println!("hello from thread {}", i);
        })
    );
}

for h in handles { h.join().unwrap() }
```

???

**explain**: all the new parts of the code

doing the same thing as before, but now we're building up a vector of handles and joining all of them. it's all a bit clunky, but so be it.

let's try running and compiling...

---

# multiple threads

```sh
corey@mac /p/tmp> rustc code.rs
corey@mac /p/tmp> ./code
hello from thread 2
hello from thread 1
hello from thread 3
hello from thread 0
corey@mac /p/tmp> ./code
hello from thread 0
hello from thread 3
hello from thread 2
hello from thread 1
```

???

**explain**: the output

as you can see, all four threads get run, in a non-deterministic order, in true asynchronous fashion

let's try bumping up the amount of concurrency here a little bit...

---

# multiple threads

```rust
let mut handles = vec![];

for i in 0..100_000 {
    handles.push(
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(1));
            println!("hello from thread {}", i);
        })
    );
}

for h in handles { h.join().unwrap() }
```

---

```sh
corey@mac /p/tmp> rustc code.rs
corey@mac /p/tmp> ./code
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error { repr: Os { code: 35, message: "Resource temporarily unavailable" } }', src/libcore/result.rs:906:4
note: Run with `RUST_BACKTRACE=1` for a backtrace.
```

???

let's return to our code

---

# multiple threads

```rust
let mut handles = vec![];

for i in 0..100_000 {
    handles.push(
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(1));
            println!("spawning thread {}", i);
        })
    );
}

for h in handles { h.join().unwrap() }
```

???

instead of printing after we sleep, let's print as soon as we start the thread...

---

# multiple threads

```rust
let mut handles = vec![];

for i in 0..100_000 {
    handles.push(
        thread::spawn(move || {
            println!("spawning thread {}", i);
            thread::sleep(Duration::from_secs(1));
        })
    );
}

for h in handles { h.join().unwrap() }
```

---

```sh
corey@mac /p/tmp [101]> ./code
spawning thread 0
spawning thread 1
spawning thread 3
spawning thread 4
spawning thread 2
...
spawning thread 2042
spawning thread 2043
spawning thread 2044
spawning thread 2045
spawning thread 2046
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error { repr: Os { code: 35, message: "Resource temporarily unavailable" } }', src/libcore/result.rs:906:4
note: Run with `RUST_BACKTRACE=1` for a backtrace.
```

???

**explain**: everything in the console

those of you with powers of two memorized might have alarms going off in your head right now

so what's going on here

---

class: center, middle

# threading models

## 1:1 threading

???

to answer that question, we first need to answer the question, "what is rust's threading model?"

---

class: center, middle

<img src="https://i.imgur.com/j4xuW9Z.png" width="100%">

???

where a language calls the operating system APIs to create threads is sometimes called 1:1, one OS thread per one language thread

you might ask, what other types of threading models are there?

---

class: center, middle

# green threads

## M:N threading

???

many programming languages provide their own special implementation of threads.

programming language provided threads are sometimes called lightweight or green threads.

these languages take a number of green threads and execute them in the context of a different number of operating system threads.

---

class: center, middle

# native threads vs. green threads

<img src="https://i.imgur.com/GtsNXX5.png" width="90%">

???

here's a visual comparing native os threads and green threads

**describe**: the visual

languages with green threads:

* go goroutines
* python w/ gevent
* lua coroutines
* julia tasks

so for example if you tried to run 100,000 goroutines, we would not hit a native thread limit since go multiplexes go routines to a fixed number of native threads.

there could be a whole talk dedicated to discussing the tradeoffs of native threads versus green threads, so i'm not going to spend very long on this slide.

if you're working with a new programming language and dealing with concurrency, it's important to know what's happening behind the scenes

prior to 1.0, rust used to have green threads

---

???

**read**: considering rust's native os threading model, we need some minimal layer of abstraction that allows us to schedule many tasks operating on a fixed number of native os threads.

---

class: center, middle

# thread pool!

---

class: center, middle

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Thread_pool.svg/1200px-Thread_pool.svg.png" width="100%">

???

A thread pool is a group of spawned threads that are ready to handle some task. 

When the program receives a new task, one of the threads in the pool will be assigned the task and will go off and process it.

The remaining threads in the pool are available to handle any other tasks that come in while the first thread is processing.

When the first thread is done processing its task, it gets returned to the pool of idle threads ready to handle a new task.

---

```rust
let mut handles = vec![];

for i in 0..100_000 {
    handles.push(
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(1));
            println!("hello from thread {}", i);
        })
    );
}

for h in handles { h.join().unwrap() }
```

???

---

```rust
let mut threadpool = ThreadPool::new(4);

for i in 0..100_000 {
    threadpool.execute(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from thread {}", i);
    });
}

threadpool.join().unwrap();
```

---

```rust
let listener =
    TcpListener::bind("127.0.0.1:8080").unwrap();

for stream in listener.incoming() {
    let stream = stream.unwrap();

    thread::spawn(|| {
        handle_connection(stream);
    });
}
```

???

more realistic example of how this problematic

one thread per incoming tcp connection

you might be thinking: 'who would ever design a system like that!'

---

<img src="https://i.imgur.com/sCR87pi.png" width="100%">

???

RFC 3875 - The Common Gateway Interface (CGI)

<em>
9.5.  Script Interference with the Server

The most common implementation of CGI invokes the script as a child
process using the same user and group as the server process.  It
should therefore be ensured that the script cannot interfere with the
server process, its configuration, documents or log files.
</em>

---

```rust
fn main() {
    let listener = TcpListener::bind("127.0.0.1:8080").unwrap();
    let pool = ThreadPool::new(4);

    for stream in listener.incoming() {
        let stream = stream.unwrap();

        pool.execute(|| {
            handle_connection(stream);
        });
    }
}
```

???

explain how this code is different than the previous iteration

---

class: center, middle

# implementation

???

**read**: so as per the last slide, now that you've seen what a basic interface is for the threadpool, i want to briefly run through how one might implement this.

---

class: center, middle

# https://doc.rust-lang.org/book/second-edition/

???

the implementation i'm walking through is virtually identical to the threadpool implementation introduced in the last chapter of the second edition of the rust programming language book.

for simplicity and readability, i cut a few corners in the implementation so the slides aren't just a wall of text.

if you get lost during this section, don't worry, you can always refer to this link for a more thorough explanation.

---

class: center, middle

# components

## threadpool

## job

## worker

---

# implementation: threadpool

```rust
pub struct ThreadPool {
    ...
}

impl ThreadPool {
    pub fn new(size: usize) -> ThreadPool {
        ...
    }

    pub fn execute<F>(&self, f: F)
        where
            F: FnOnce() + Send + 'static
    {
        ...
    }
}
```

???

the traits on `where` clause are the same traits specified by the standard library's `thread::spawn` function we saw earlier

**read**: `FnOnce`: in rust, closures can capture their environment in a few different ways. in particular, every closure in rust either implements the `Fn` trait, the `FnMut` trait, or the `FnOnce` trait. `FnOnce` indicates the closure is only called once and moves ownership of captured values into the closure.
[1](https://doc.rust-lang.org/book/second-edition/ch13-01-closures.html#closures-can-capture-their-environment)

**read**: `Send` indicates the closure is able to safely be sent to another thread
[1](https://doc.rust-lang.org/beta/nomicon/send-and-sync.html)

**read**: `'static` indicates that any references used within the closure must have a static lifetime, which more or less means if you use a reference in the closure, it must be constant or static.

---

# implementation: threadpool

```rust
impl ThreadPool {
    pub fn new(size: usize) -> ThreadPool {
        let (sender, receiver) = mpsc::channel();

        let mut workers = vec![];

        for id in 0..size {
            workers.push(Worker::new(receiver.clone()));
        }

        ThreadPool {
            workers,
            sender,
        }
    }
}
```

???

mpsc: multi producer single consumer

---

# implementation: threadpool

```rust
impl ThreadPool {
    pub fn execute<F>(&self, f: F)
        where
            F: FnOnce() + Send + 'static
    {
        let job = Box::new(f);

        self.sender.send(job).unwrap();
    }
}
```

---

# implementation: job

```rust
type Job = Box<FnOnce() + Send + 'static>;
```

???

**read**: because the compiler doesn't know the size of the closure at compile time, since closure size can vary, we need to move the closure onto the heap before we can pass it through channels to other threads.

---

# implementation: worker

```rust
struct Worker {
    ...
}

impl Worker {
    fn new(receiver: mpsc::Receiver<Job>)
        -> Worker
    {
        ...
    }
}
```

---

# implementation: worker

```rust
struct Worker {
    handle: thread::JoinHandle<()>,
}
```

---

# implementation: worker

```rust
impl Worker {
    fn new(receiver: mpsc::Receiver<Job>)
        -> Worker
    {
        let handle = thread::spawn(move || {
            loop {
                let job = receiver.recv().unwrap();
                job();
            }
        });

        Worker { handle }
    }
}
```

???

**explain**: the code

**something i simplified**: Receiver is not allowed to be shared across different threads (because !Send), so it needs to be wrapped in a mutex to ensure it's only used by one thread at a time

---

???

and that's it, that's how you implement a threadpool

like i said earlier, if you want a much more thorough explanation about the implementation, i strongly recommend taking a look at the book.

---

class: center, middle

# usage

---

class: center, middle

# threadpool

# scoped-pool

# scoped_threadpool

# rayon

???

**explain**: history of each

**threadpool**: used to be TaskPool in the std before 1.0

---

# rayon

```rust
use rayon::prelude::*;

fn sum_of_squares(input: &[i32]) -> i32 {
    input.par_iter()
         .map(|&i| i * i)
         .sum()
}
```

```rust
use rayon::prelude::*;

fn increment_all(input: &mut [i32]) {
    input.par_iter_mut()
         .for_each(|p| *p += 1);
}
```

---

class: center, middle

```rust
rayon::ThreadPool
```

---

class: small

# rayon

```rust
/// Returns a handle to the global thread pool. This is the pool
/// that Rayon will use by default when you perform a `join()` or
/// `scope()` operation, if no other thread-pool is installed. If
/// no global thread-pool has yet been started when this function
/// is called, then the global thread-pool will be created (with
/// the default configuration). If you wish to configure the
/// global thread-pool differently, then you can use [the
/// `rayon::initialize()` function][f] to do so.
///
/// [f]: fn.initialize.html
pub fn global() -> &'static Arc<ThreadPool> {
    lazy_static! {
        static ref DEFAULT_THREAD_POOL: Arc<ThreadPool> =
            Arc::new(ThreadPool { registry: Registry::global() });
    }

    &DEFAULT_THREAD_POOL
}
```

???

**describe**: what is good about a static threadpool

---

class: center, middle

```rust
rayon::spawn(|| {
    // ...
})
```

???

rayon is the primary threadpool used in servo, and parts of servo are shipping in firefox literally this next tuesday, so as far as i understand rayon will be in firefox next week

---

```rust
let mut vec = vec![0, 1, 2, 3, 4, 5, 6, 7];

// Use the threads as scoped threads that can
// reference anything outside this closure
rayon::scoped(|scoped| {
    // Create references to each element in the vector ...
    for e in &mut vec {
        // ... and add 1 to it in a seperate thread
        scoped.execute(move || {
            *e += 1;
        });
    }
});

assert_eq!(vec, vec![1, 2, 3, 4, 5, 6, 7, 8]);
```

.footnote[https://github.com/rayon-rs/rayon/blob/master/rayon-core/src/thread_pool/mod.rs]

---

questions?

???

threads vs. processes: The typical difference is that threads (of the same process) run in a shared memory space, while processes run in separate memory spaces.

---

thanks!
    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
