<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url('https://fonts.googleapis.com/css?family=Lato|Inconsolata');

      body {
        font-family: 'Lato';
      }

      .remark-slide-content h1, h2, h3, h4, h5, h6 {
        font-family: 'Lato';
        font-weight: normal;
      }

      .remark-slide-content h1 {
        font-size: 3rem;
      }

      .remark-slide-content h2 {
        font-size: 2.4rem;
      }

      .remark-code, .remark-inline-code {
        font-family: 'Inconsolata';
        font-size: 1.6rem;
        white-space: pre-wrap;
      }

      .footnote {
        font-size: 0.8em;
      }

      .small .remark-code {
        font-size: 110%;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# threads

# threadpools

# implementation

# usage

???

hello everyone, i'm going to be talking today mainly about threadpools in rust. in particular starting out by introducing...

# threads

* how to use them in rust

# threadpools

* why we need them

# implementation

* basic implementation

# usage

* brief overview of existing threadpool implementations

---

class: center, middle

# who

## corey farwell

## rwell.org

## @frewsxcv

???

who am i?

**corey farwell**

backend software engineer at kickstarter

ruby / rails

started using rust late 2014

contributed to various projects over the years

rust documentation team (w/ steve)

**rwell.org**

there's not much on it so don't bother going to it

**frewsxcv**

unpronouncible online handle

twitter github

---

class: center, middle

# https://slides.rwell.org

???

**read**: in case you want to follow along, here's a url to the slides

---

class: center, middle

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Rust_programming_language_black_logo.svg/1024px-Rust_programming_language_black_logo.svg.png" width="30%">

# safe

# concurrent

# practical


???

let's begin. rust is commonly advertised as a

* safe
* concurrent
* and practical systems language

the focus of my talk today is on concurrency.

---

class: center, middle

# concurrent

???

wikipedia defines concurrent computing to be:

*a form of computing in which several computations are executed during overlapping time periodsâ€”concurrentlyâ€”instead of sequentially*

what is the basic building block of concurrency in rust?

---

class: center, middle

# threads!

---

# threads

```rust
use std::thread;
use std::time::Duration;

fn main() {
    thread::spawn(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from a thread! ðŸ‘‹")
    });
}
```

???

**explain**: go through every bit of the code

lets see what happens when we compile and run this...

---

# threads

```rust
use std::thread;
use std::time::Duration;

fn main() {
    thread::spawn(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from a thread! ðŸ‘‹")
    });
}
```

```sh
corey@mac /p/tmp> rustc code.rs
corey@mac /p/tmp> ./code
corey@mac /p/tmp>
```

???

what happened? where's our output?

some of you in the audience might have caught it earlier.

in the main thread, we called `thread::spawn` and a child thread spawned. but then the main thread reached the end of the function. when that happens, the main thread kills all child threads and the child thread never printed the output.

so how do we get the main thread to wait for the child thread to finish?

---

# threads

```rust
use std::thread;
use std::time::Duration;

fn main() {
    let handle = thread::spawn(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from a thread! ðŸ‘‹")
    });

    handle.join().unwrap();
}
```

???

`thread::spawn` returns a `JoinHandle` struct.

the `JoinHandle` struct allows us to block the main thread and wait for the child thread to complete via the `join` method.

now when we run it...

---

# threads

```rust
use std::thread;
use std::time::Duration;

fn main() {
    let handle = thread::spawn(|| {
        thread::sleep(Duration::from_secs(1));
        println!("hello from a thread! ðŸ‘‹")
    });

    handle.join().unwrap();
}
```

```sh
corey@mac /p/tmp> rustc code.rs
corey@mac /p/tmp> ./code
hello from a thread! ðŸ‘‹
```

???

cool, looks like it worked

so now that we've mastered spawing a single thread, let's try spawning multiple

---

# multiple threads

```rust
let mut handles = vec![];

for i in 0..4 {
    handles.push(
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(1));
            println!("hello from thread {}", i);
        })
    );
}

for h in handles { h.join().unwrap() }
```

???

**explain**: all the new parts of the code

doing the same thing as before, but now we're building up a vector of handles and joining all of them. it's all a bit clunky, but so be it.

let's try running and compiling...

---

# multiple threads

```sh
corey@mac /p/tmp> rustc code.rs
corey@mac /p/tmp> ./code
hello from thread 2
hello from thread 1
hello from thread 3
hello from thread 0
corey@mac /p/tmp> ./code
hello from thread 0
hello from thread 3
hello from thread 2
hello from thread 1
```

???

**explain**: the output

as you can see, all four threads get run, in a non-deterministic order, in true asynchronous fashion

four tasks getting run is not very impressive, let's try bumping up that number a small amount...

---

# multiple threads

```rust
let mut handles = vec![];

for i in 0..100_000 {
    handles.push(
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(1));
            println!("hello from thread {}", i);
        })
    );
}

for h in handles { h.join().unwrap() }
```

???

**explain**: the differences in the code

---

```sh
corey@mac /p/tmp> rustc code.rs
corey@mac /p/tmp> ./code
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error { repr: Os { code: 35, message: "Resource temporarily unavailable" } }', src/libcore/result.rs:906:4
note: Run with `RUST_BACKTRACE=1` for a backtrace.
```

???

**read**: the error message

let's return to our code

---

# multiple threads

```rust
let mut handles = vec![];

for i in 0..100_000 {
    handles.push(
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(1));
            println!("spawning thread {}", i);
        })
    );
}

for h in handles { h.join().unwrap() }
```

???

instead of printing after we sleep, let's print as soon as we start the thread...

---

# multiple threads

```rust
let mut handles = vec![];

for i in 0..100_000 {
    handles.push(
        thread::spawn(move || {
            println!("spawning thread {}", i);
            thread::sleep(Duration::from_secs(1));
        })
    );
}

for h in handles { h.join().unwrap() }
```

---

```sh
corey@mac /p/tmp [101]> ./code
spawning thread 0
spawning thread 1
spawning thread 3
spawning thread 4
spawning thread 2
...
spawning thread 2042
spawning thread 2043
spawning thread 2044
spawning thread 2045
spawning thread 2046
thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error { repr: Os { code: 35, message: "Resource temporarily unavailable" } }', src/libcore/result.rs:906:4
note: Run with `RUST_BACKTRACE=1` for a backtrace.
```

???

**explain**: everything in the console

those of you with powers of two memorized might have alarms going off in your head right now

so what's going on here

---

class: center, middle

# threading models

???

to answer that question, we first need to answer the question, "what is rust's threading model?"

among other things, a programming language's threading model specifies how language defined threads interact with native operating system threads

---

class: center, middle

<img src="https://i.imgur.com/j4xuW9Z.png" width="100%">

???

**read**: off the image

so in essence, whenever a rust thread is created, a corresponding native thread is created. because there is a 1:1 relationship here, this sort of threading model is often referred to as 1:1 threading.

so in our previous example, when we tried to spin up 100,000 rust threads, because of rust's threading model, we're also telling macOS to spin up 100,000 threads, and it so happens that (at least on my machine) macOS doesn't want me spinning up more than 2048 threads.

so now we have an understanding of what this 1:1 threading is, you might ask, what other types of threading models are there?

---

class: center, middle

# green threads

## M:N threading

???

many programming languages provide their own special implementation of threads.

programming language provided threads are sometimes called lightweight or green threads.

these languages take a number of green threads and execute them in the context of a different number of operating system threads.

**mention**: M:N

---

class: center, middle

# native threads vs. green threads

<img src="https://i.imgur.com/GtsNXX5.png" width="90%">

???

here's a crappy visual i through together comparing native os threads and green threads

**describe**: the visual

languages with green threads:

* go goroutines
* python w/ gevent
* lua coroutines
* julia tasks
* prior to 1.0, rust used to have green threads

so for example if you tried to run 100,000 goroutines, we would not hit a native thread limit since go multiplexes go routines to a fixed number of native threads.

there are a ton of tradeoffs when deciding if one's programming language should use native threads or green threads. how different languages actually implement green threading is a bit fuzzy to me so it's hard for me personally to make good direct comparisons, soooooooo don't expect me to answer any super indepth questions about this.

the important takeaway here is, if you're working with concurrency in any programming language or library, it's important to consider the threading model. is it okay for me to execute 10,000 coroutines, threads, goroutines, tasks.

---

???

**read**: considering rust's native os threading model, we need some minimal layer of abstraction that allows us to schedule many tasks (potentially 100,000 takss) operating on a fixed number of native os threads.

---

class: center, middle

# thread pool!

---

class: center, middle

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Thread_pool.svg/1200px-Thread_pool.svg.png" width="100%">

???

A thread pool is a group of spawned threads that are ready to handle some task. 

When the program receives a new task, one of the threads in the pool will be assigned the task and will go off and process it.

The remaining threads in the pool are available to handle any other tasks that come in while the first thread is processing.

When the first thread is done processing its task, it gets returned to the pool of idle threads ready to handle a new task.

so going back to our earlier problematic code...

---

```rust
let mut handles = vec![];

for i in 0..100_000 {
    handles.push(
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(1));
            println!("hello from thread {}", i);
        })
    );
}

for h in handles { h.join().unwrap() }
```

???

---

```rust
let mut threadpool = ThreadPool::new(4);

for i in 0..100_000 {
    threadpool.execute(move || {
        thread::sleep(Duration::from_secs(1));
        println!("hello from thread {}", i);
    });
}

threadpool.join().unwrap();
```

???

**explain**: the code

---

class: center, middle

# implementation

???

**read**: so as per the last slide, now that you've seen what a basic interface is for the threadpool, i want to briefly run through how one might implement this.

---

class: center, middle

# https://doc.rust-lang.org/book/second-edition/

???

the implementation i'm walking through is virtually identical to the threadpool implementation introduced in the last chapter of the second edition of the rust programming language book.

for simplicity and readability, i cut a few corners in the implementation so the slides aren't just a wall of text.

if you get lost during this section, don't worry, you can always refer to the book for a more thorough explanation.

---

class: center, middle

# components

## threadpool

## job

## worker

---

class: center, middle

<img src="https://i.imgur.com/ERhFzxz.png" width="90%">

???

**explain** the relationships between all these concepts. what creates what

* job is just rust closure
* threadpool sends out job to all the workers via a channel
* each of the workers listen on the channel and the jobs and run them on a thread

---

# implementation: threadpool

```rust
pub struct ThreadPool {
    ...
}

impl ThreadPool {
    pub fn new(size: usize) -> ThreadPool {
        ...
    }

    pub fn execute<F>(&self, f: F)
        where
            F: FnOnce() + Send + 'static
    {
        ...
    }
}
```

???

the traits on `where` clause are the same traits specified by the standard library's `thread::spawn` function we saw earlier

**read**: `FnOnce`: in rust, closures can capture their environment in a few different ways. in particular, every closure in rust either implements the `Fn` trait, the `FnMut` trait, or the `FnOnce` trait. `FnOnce` indicates the closure is only called once and moves ownership of captured values into the closure.
[1](https://doc.rust-lang.org/book/second-edition/ch13-01-closures.html#closures-can-capture-their-environment)

**read**: `Send` indicates the closure is able to safely be sent to another thread
[1](https://doc.rust-lang.org/beta/nomicon/send-and-sync.html)

**read**: `'static` indicates that any references used within the closure must have a static lifetime, which more or less means if you use a reference in the closure, it must be constant or static.

---

# implementation: threadpool

```rust
impl ThreadPool {
    pub fn new(size: usize) -> ThreadPool {
        let (sender, receiver) = mpsc::channel();

        let mut workers = vec![];

        for id in 0..size {
            workers.push(Worker::new(receiver.clone()));
        }

        ThreadPool {
            workers,
            sender,
        }
    }
}
```

???

mpsc: multi producer single consumer

---

# implementation: threadpool

```rust
impl ThreadPool {
    pub fn execute<F>(&self, f: F)
        where
            F: FnOnce() + Send + 'static
    {
        let job = Box::new(f);

        self.sender.send(job).unwrap();
    }
}
```

---

# implementation: job

```rust
type Job = Box<FnOnce() + Send + 'static>;
```

???

**read**: because the compiler doesn't know the size of the closure at compile time, since closure size can vary, we need to move the closure onto the heap before we can pass it through channels to other threads.

---

# implementation: worker

```rust
struct Worker {
    ...
}

impl Worker {
    fn new(receiver: mpsc::Receiver<Job>)
        -> Worker
    {
        ...
    }
}
```

---

# implementation: worker

```rust
struct Worker {
    handle: thread::JoinHandle<()>,
}
```

---

# implementation: worker

```rust
impl Worker {
    fn new(receiver: mpsc::Receiver<Job>)
        -> Worker
    {
        let handle = thread::spawn(move || {
            loop {
                let job = receiver.recv().unwrap();
                job();
            }
        });

        Worker { handle }
    }
}
```

???

**explain**: the code

**something i simplified**: Receiver is not allowed to be shared across different threads (because !Send), so it needs to be wrapped in a mutex to ensure it's only used by one thread at a time

---

???

and that's it, that's how you implement a threadpool

like i said earlier, if you want a much more thorough explanation about the implementation, i strongly recommend taking a look at the book.

---

class: center, middle

# usage

---

class: center, middle

# threadpool

# scoped-pool

# scoped_threadpool

# rayon

???

**explain**: history of each

**threadpool**: used to be TaskPool in the std before 1.0

between these four crates, if you're looking to use a threadpool, i would suggest using rayon.

---

# rayon

```rust
use rayon::prelude::*;

fn sum_of_squares(input: &[i32]) -> i32 {
    input.par_iter()
         .map(|&i| i * i)
         .sum()
}
```

```rust
use rayon::prelude::*;

fn increment_all(input: &mut [i32]) {
    input.par_iter_mut()
         .for_each(|p| *p += 1);
}
```

---

# rayon

```rust
rayon::ThreadPool::new();
```

---

class: small

# rayon

```rust
/// Returns a handle to the global thread pool. This is the pool
/// that Rayon will use by default when you perform a `join()` or
/// `scope()` operation, if no other thread-pool is installed. If
/// no global thread-pool has yet been started when this function
/// is called, then the global thread-pool will be created (with
/// the default configuration). If you wish to configure the
/// global thread-pool differently, then you can use [the
/// `rayon::initialize()` function][f] to do so.
///
/// [f]: fn.initialize.html
pub fn global() -> &'static Arc<ThreadPool> {
    lazy_static! {
        static ref DEFAULT_THREAD_POOL: Arc<ThreadPool> =
            Arc::new(ThreadPool { registry: Registry::global() });
    }

    &DEFAULT_THREAD_POOL
}
```

???

**describe**: what is good about a static threadpool

---

# rayon

```rust
rayon::ThreadPool::global().spawn(|| {
    // ...
})
```

```rust
rayon::spawn(|| {
    // ...
})
```

???

rayon is the primary threadpool used in servo, and parts of servo are shipping in firefox literally this next tuesday, so as far as i understand rayon will be in firefox next week

---

class: center, middle

# extra credit: scoped pools

---

# scoped pools

```rust
let mut vec = vec![0, 1, 2, 3, 4, 5, 6, 7];
let pool = ThreadPool::new(4);

for e in &mut vec {
    pool.execute(move || {
        *e += 1;
    });
}

pool.join().unwrap();
println!("{:?}", vec);
```

???

imagine this scenario...

**explain**: the code

This task will run in the implicit, global scope, which means that it may outlast the current stack frame -- therefore, it cannot capture any references onto the stack.

explain**: the code and why it doesn't compile

---

# scoped pools

class: small

```sh
error[E0597]: `vec` does not live long enough
--> src/main.rs:9:19
 |
 |     for e in &mut vec {
 |                   ^^^ does not live long enough
...
 | }
 | - borrowed value only lives until here
 |
 = note: borrowed value must be valid for the static lifetime...
 ```

---

# scoped pools

```rust
let mut vec = vec![0, 1, 2, 3, 4, 5, 6, 7];

rayon::scoped(|scoped| {
    for e in &mut vec {
        scoped.execute(move || {
            *e += 1;
        });
    }
});

println!("{:?}", vec);
```

---

class: center, middle

# questions?

???

threads vs. processes: The typical difference is that threads (of the same process) run in a shared memory space, while processes run in separate memory spaces.

---

class: center, middle

# thanks!
    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
